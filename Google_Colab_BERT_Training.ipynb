{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Requirements"
      ],
      "metadata": {
        "id": "VNvdzN91MZNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4buqSIP6L-9m"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft\n",
        "!pip install -q peft evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, DataCollatorWithPadding, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "import evaluate\n",
        "from peft import LoraConfig, TaskType, get_peft_model"
      ],
      "metadata": {
        "id": "jzvkcgkOMEOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "df = pd.read_csv(\"/content/final_dataset.csv\") # can be downloaded from GitHub repo, contains 9.5k samples\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# create datasets for training, validation, and testing\n",
        "\n",
        "temp_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(temp_df, test_size=0.125, stratify=temp_df[\"label\"], random_state=73) # not sure if this split is right\n",
        "\n",
        "# create datasets from dataframes\n",
        "train_dataset = Dataset.from_pandas(train_df[['input', 'label']].reset_index(drop=True))\n",
        "val_dataset = Dataset.from_pandas(val_df[['input', 'label']].reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df[['input', 'label']].reset_index(drop=True))\n",
        "\n",
        "def tokenize_func(examples):\n",
        "  return tokenizer(examples[\"input\"], truncation=True, max_length=128, padding=\"max_length\")\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_func, batched=True, remove_columns=\"input\")\n",
        "tokenized_val = val_dataset.map(tokenize_func, batched=True, remove_columns=\"input\")\n",
        "tokenized_test = test_dataset.map(tokenize_func, batched=True, remove_columns=\"input\")"
      ],
      "metadata": {
        "id": "jPFjPgJPMFMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Setup"
      ],
      "metadata": {
        "id": "-19Z8n4tMjvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # used claude to write code to calculate multiple training metrics\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='binary')\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='binary')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='binary')\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'precision': precision['precision'],\n",
        "        'recall': recall['recall'],\n",
        "        'f1': f1['f1'],\n",
        "        # for printed confusion matrix\n",
        "        'true_positives': tp,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn,\n",
        "        'true_negatives': tn,\n",
        "    }"
      ],
      "metadata": {
        "id": "RHn7FbEzMjcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Setup"
      ],
      "metadata": {
        "id": "QFuQdtu6Nu6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define lora config (from hyperparameter grid search)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.2,\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# weighted trainer code from Claude to improve model training w/ class imbalance\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculate class weights\n",
        "        n_samples = len(train_df)\n",
        "        n_positive = train_df['label'].sum()\n",
        "        n_negative = n_samples - n_positive\n",
        "        pos_weight = n_negative / n_positive * 1.3\n",
        "\n",
        "        # Fix: Use .float() to ensure float32\n",
        "        loss_fct = nn.CrossEntropyLoss(\n",
        "            weight=torch.tensor([1.0, pos_weight], dtype=torch.float32).to(logits.device)\n",
        "        )\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "rZts8YNpMu1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train model\n",
        "def train_full_model(number, train_dataset, val_dataset, tokenizer, lora_config):\n",
        "  # define model\n",
        "  model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=f\"./model_train_f_{number}\",\n",
        "      eval_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      save_total_limit=None, # currently saves models from every epoch\n",
        "      num_train_epochs=10,\n",
        "      per_device_train_batch_size=16,\n",
        "      per_device_eval_batch_size=32,\n",
        "      learning_rate=1e-4,\n",
        "\n",
        "      # claude code to improve model training by decreasing memorization and stabilizing learning\n",
        "      weight_decay=0.01,\n",
        "      warmup_ratio=0.1,\n",
        "\n",
        "      # claude code to prevent unecessary logging\n",
        "      report_to=[],\n",
        "      logging_strategy=\"no\",\n",
        "\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"recall\",\n",
        "      greater_is_better=True\n",
        "  )\n",
        "\n",
        "  # claude code to implement early stopping\n",
        "  early_stopping = EarlyStoppingCallback(\n",
        "      early_stopping_patience=3\n",
        "  )\n",
        "\n",
        "\n",
        "  # create trainer from weighted trainer class\n",
        "  trainer = WeightedTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "      tokenizer=tokenizer,\n",
        "      callbacks=[early_stopping],\n",
        "  )\n",
        "\n",
        "  # train\n",
        "  trainer.train()\n",
        "\n",
        "  # final val metrics\n",
        "  eval_results = trainer.evaluate()\n",
        "\n",
        "  model.save_pretrained(f\"./final_model_{number}\") # for later reference\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "_rbxykzYNApT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "opF5yaPONyGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_model(1, tokenized_train, tokenized_val, tokenizer, lora_config)"
      ],
      "metadata": {
        "id": "zySJBCXJNp46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}